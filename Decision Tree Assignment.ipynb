{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1720e59f-3614-44a3-a896-764ff5bcf42d",
   "metadata": {},
   "source": [
    "## Decision Tree | **Assignment**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e63c00-c751-42f0-8a46-bd16ed98ea68",
   "metadata": {},
   "source": [
    "### 1. What is a Decision Tree, and how does it work in the context of classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5069207-c7a3-40a4-843b-5bfd96cf7742",
   "metadata": {},
   "source": [
    "#### A Decision Tree is a supervised learning algorithm used for classification and regression tasks. In classification, it helps predict which category a given data point belongs to by learning decision rules from the data features.\n",
    "How It Works (Step-by-Step for Classification)\n",
    "- Root Node\n",
    "    - Starts with the entire dataset.\n",
    "    - Chooses the best feature to split on using impurity metrics like Gini or Entropy.\n",
    "- Splitting Criteria\n",
    "    - At each node, the algorithm selects the feature and threshold that maximizes class separation (e.g., highest Information Gain or Gini reduction).\n",
    "- Branching\n",
    "    - Data is partitioned into subsets.\n",
    "    - Each subset forms a new branch with more specific conditions.\n",
    "- Leaf Node\n",
    "    - Splitting stops when: \n",
    "        - All samples belong to the same class.\n",
    "        - Impurity is minimal.\n",
    "        - Predefined constraints are reached (e.g., max depth).\n",
    "    - Final prediction is made based on majority class in that leaf.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad264346-7bd3-42cc-98c2-31b9ecf94273",
   "metadata": {},
   "source": [
    "### 2. Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097105f1-b3f0-4e87-8e7f-57498a95f3f8",
   "metadata": {},
   "source": [
    "#### Gini Impurity measures the probability that a randomly chosen sample from the node would be incorrectly classified if labeled randomly according to the distribution of labels in that node.\n",
    "Formula:  \t$Gini=1-\\sum_{i=1}{(\\ p}_{i\\ })2$<br>\n",
    "\t$\\left(p_i\\right)$ is the probability of class ($i$) in the node<br>\n",
    "Example: A node has:\n",
    "- 60% Class A\n",
    "- 40% Class B\n",
    "<br>Gini = (1 - (0.62 + 0.42) = 0.48)<br>\n",
    "\n",
    "Entropy comes from information theory. It measures the amount of uncertainty or disorder at a node.<br>\n",
    "Formula:\t\t$Entropy=-\\sum_{i=1}^{N}p_i\\cdot\\log_2{\\left(p_i\\right)}$\n",
    "<br>Example: Same distribution:<br>\n",
    "Entropy=$-\\left(0.6\\cdot\\log_2{\\left(0.6\\right)}+0.4\\cdot\\log_2{\\left(0.4\\right)}\\right)\\approx0.97$<br>\n",
    "Impact on Tree Splits<br>\n",
    "Both metrics are used to evaluate which feature and threshold split the node most effectively:\n",
    "- Lower impurity after the split -> better feature\n",
    "\n",
    "Algorithms try all possible splits, compute resulting impurity, and select the one that maximizes purity in child nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e090504-5ffc-4bb3-85dd-7ad410d5b8ea",
   "metadata": {},
   "source": [
    "### 3. What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ea02b8-0b1d-4b40-af1c-7c970749d355",
   "metadata": {},
   "source": [
    "#### Pre-Pruning vs Post-Pruning\n",
    "\n",
    "| Feature           | **Pre-Pruning**                             | **Post-Pruning**                            |\n",
    "|-------------------|---------------------------------------------|---------------------------------------------|\n",
    "| Timing            | Happens *while* building the tree           | Happens *after* the tree is fully grown     |\n",
    "| Method           | Stops splitting if conditions fail          | Removes subtrees based on validation checks |\n",
    "| Criteria Used    | Max depth, min samples, Gini/Entropy gain   | Validation accuracy, complexity trade-off   |\n",
    "| Outcome          | May prevent overfitting early               | Cleans up a complex model post hoc          |\n",
    "\n",
    "\n",
    "#### Practical Advantages\n",
    "\n",
    "- **Pre-Pruning Advantage**  \n",
    "  *Speeds up training time* — By halting unnecessary splits early, it reduces computational cost, especially useful for large datasets or real-time systems.\n",
    "\n",
    "- **Post-Pruning Advantage**  \n",
    "  *Improves generalization* — By trimming branches that don’t boost validation accuracy, it sharpens the model’s ability to perform well on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68f9b2a-48a5-4ff4-a109-24ec15f8fcbf",
   "metadata": {},
   "source": [
    "### 4. What is Information Gain in Decision Trees, and why is it important for choosing the best split?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81084517-8433-4fe2-ac36-54c1878a7d14",
   "metadata": {},
   "source": [
    "**Information Gain (IG)** measures the reduction in entropy when a dataset is split on a particular feature.\n",
    "\n",
    "Entropy quantifies uncertainty — so IG tells us how *much that uncertainty drops* when we make a split.\n",
    "\n",
    "#### Formula:\n",
    "```\n",
    "Information Gain = Entropy(parent) - ∑ (Weighted Entropy(children))\n",
    "```\n",
    "We compute the total entropy before the split, then subtract the weighted sum of the entropies after splitting the data.\n",
    "\n",
    "#### Why Is It Important?\n",
    "\n",
    "Because it acts like a magnet for the **best feature to split on**.\n",
    "\n",
    "- If IG is high → the split creates purer child nodes → great candidate for a split!\n",
    "- If IG is low → the feature doesn't reduce uncertainty much → not ideal.\n",
    "\n",
    "In short: **Higher Information Gain = More confident and meaningful split**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79457db3-4e52-47a5-802f-c6ec6ff25e2f",
   "metadata": {},
   "source": [
    "### 5. What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8a5284-3f50-4f9a-8490-22e74b73d836",
   "metadata": {},
   "source": [
    "### Common Real-World Applications of Decision Trees\n",
    "\n",
    "#### Healthcare\n",
    "- **Disease diagnosis**: Classify patients based on symptoms, lab results, and history.\n",
    "- **Triage systems**: Prioritize emergency cases using decision paths.\n",
    "- **Treatment recommendations**: Suggest therapies based on patient profiles.\n",
    "\n",
    "#### Business & Finance\n",
    "- **Loan approval**: Assess creditworthiness using income, credit score, and history.\n",
    "- **Fraud detection**: Flag suspicious transactions by learning patterns.\n",
    "- **Investment decisions**: Evaluate risk profiles and market conditions.\n",
    "\n",
    "#### Retail & Marketing\n",
    "- **Customer segmentation**: Group users by behavior, demographics, or purchase history.\n",
    "- **Churn prediction**: Identify customers likely to leave and trigger retention strategies.\n",
    "- **Product recommendation**: Suggest items based on decision paths from past purchases.\n",
    "\n",
    "#### Education\n",
    "- **Student performance prediction**: Use attendance, grades, and engagement to forecast outcomes.\n",
    "- **Adaptive learning systems**: Tailor content based on learner responses.\n",
    "\n",
    "#### Engineering & Operations\n",
    "- **Quality control**: Classify defective vs. non-defective items.\n",
    "- **Maintenance scheduling**: Predict equipment failure based on usage and sensor data.\n",
    "\n",
    "\n",
    "\n",
    "#### Advantages of Decision Trees:\n",
    "- **Interpretability** | Easy to visualize and explain to non-technical stakeholders.\n",
    "- **Handles mixed data types** | Works with both categorical and numerical features.\n",
    "- **Minimal preprocessing** | No need for scaling or normalization.\n",
    "- **Captures non-linear relationships** | Splits can model complex decision boundaries.\n",
    "- **Feature importance** | Highlights which variables drive decisions.\n",
    "- **Robust to missing values** | Can handle gaps without imputation.\n",
    "\n",
    "\n",
    "\n",
    "#### Limitations of Decision Trees:\n",
    "- **Overfitting** | Deep trees may memorize training data, hurting generalization.\n",
    "- **Instability** | Small data changes can lead to very different trees.\n",
    "- **Bias toward dominant features** | Features with many levels may dominate splits.\n",
    "- **Greedy splitting** | Locally optimal splits may not yield the best global structure.\n",
    "- **Poor extrapolation** | Regression trees struggle with continuous trends.\n",
    "- **Computational cost** | Large datasets can lead to deep, complex trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba80a5e-10f3-4300-aef5-ca4207d99ab1",
   "metadata": {},
   "source": [
    "### 6. Write a Python program to:\n",
    "- Load the Iris Dataset\n",
    "- Train a Decision Tree Classifier using the Gini criterion\n",
    "- Print the model’s accuracy and feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "81f0af15-b10a-456a-937e-9c3f5a69cc6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.97\n",
      "\n",
      "Feature Importances:\n",
      "sepal length (cm)   : 0.017\n",
      "sepal width (cm)    : 0.000\n",
      "petal length (cm)   : 0.517\n",
      "petal width (cm)    : 0.467\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "x = iris.data\n",
    "y = iris.target\n",
    "feature_names = iris.feature_names\n",
    "target_names = iris.target_names\n",
    "\n",
    "# Split the data into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state=4\n",
    ")\n",
    "\n",
    "# Train a Decision Tree Classifier using Gini criterion\n",
    "clf = DecisionTreeClassifier(criterion='gini', random_state=4)\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "# Predict and print accuracy\n",
    "y_pred = clf.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Print feature importances\n",
    "print(\"\\nFeature Importances:\")\n",
    "for feature, importance in zip(feature_names, clf.feature_importances_):\n",
    "    print(f\"{feature:20}: {importance:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a28fc2-54dd-4ca8-b0ec-7aa1d3a57b07",
   "metadata": {},
   "source": [
    "### 7. Write a Python program to:\n",
    "- Load the Iris Dataset\n",
    "- Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4259c55b-61d1-4853-a94b-650a3ce55522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fully-grown Tree Accuracy: 0.97\n",
      "Shallow Tree Accuracy (max_depth=3): 0.97\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "iris = load_iris()\n",
    "x, y = iris.data, iris.target\n",
    "\n",
    "# Split into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state=1\n",
    ")\n",
    "\n",
    "# Fully-grown tree (no max_depth constraint)\n",
    "full_tree = DecisionTreeClassifier(criterion='gini', random_state=1)\n",
    "full_tree.fit(x_train, y_train)\n",
    "full_pred = full_tree.predict(x_test)\n",
    "full_accuracy = accuracy_score(y_test, full_pred)\n",
    "\n",
    "# Limited-depth tree (max_depth=3)\n",
    "shallow_tree = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=1)\n",
    "shallow_tree.fit(x_train, y_train)\n",
    "shallow_pred = shallow_tree.predict(x_test)\n",
    "shallow_accuracy = accuracy_score(y_test, shallow_pred)\n",
    "\n",
    "# Output comparison\n",
    "print(f\"Fully-grown Tree Accuracy: {full_accuracy:.2f}\")\n",
    "print(f\"Shallow Tree Accuracy (max_depth=3): {shallow_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3fa53f-d2b2-4325-8add-9d7fd25e0b22",
   "metadata": {},
   "source": [
    "### 8. Write a Python program to:\n",
    "- Load the Boston Housing Dataset\n",
    "- Train a Decision Tree Regressor\n",
    "- Print the Mean Squared Error (MSE) and feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9ba0d7d7-eb75-4934-9d93-dbb4e4a31c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.50\n",
      "\n",
      "Feature Importances:\n",
      "MedInc              : 0.529\n",
      "HouseAge            : 0.052\n",
      "AveRooms            : 0.053\n",
      "AveBedrms           : 0.029\n",
      "Population          : 0.031\n",
      "AveOccup            : 0.131\n",
      "Latitude            : 0.094\n",
      "Longitude           : 0.083\n"
     ]
    }
   ],
   "source": [
    "#Boston Housing Dataset is removed from Sci-Kit Learning, so I'm making making model on California Housing Dataset\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load California Housing dataset\n",
    "housing = fetch_california_housing()\n",
    "X = housing.data\n",
    "y = housing.target\n",
    "feature_names = housing.feature_names\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train Decision Tree Regressor\n",
    "regressor = DecisionTreeRegressor(random_state=42)\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict and calculate MSE\n",
    "y_pred = regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "\n",
    "# Feature Importances\n",
    "print(\"\\nFeature Importances:\")\n",
    "for name, importance in zip(feature_names, regressor.feature_importances_):\n",
    "    print(f\"{name:20}: {importance:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99a5461-7324-4f12-b124-874af10167a2",
   "metadata": {},
   "source": [
    "### 9. Write a Python program to:\n",
    "- Load the Iris Dataset\n",
    "- Tune the Decision Tree’s max_depth and min_samples_split using GridSearchCV\n",
    "- Print the best parameters and the resulting model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ec275e9e-e35b-4ee4-aad6-a42ef0f2d723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
      "Test Set Accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=1\n",
    ")\n",
    "\n",
    "# Create a Decision Tree Classifier\n",
    "dtree = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [2, 3, 4, 5, 6],\n",
    "    'min_samples_split': [2, 4, 6, 8]\n",
    "}\n",
    "\n",
    "# GridSearchCV for hyperparameter tuning\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=dtree,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Test set accuracy using best estimator\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Set Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab24e30a-22b2-47dc-89d6-741ad7f8c823",
   "metadata": {},
   "source": [
    "### 10. Imagine you’re working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values. Explain the step-by-step process you would follow to: \n",
    "- Handle the missing values \n",
    "- Encode the categorical features \n",
    "- Train a Decision Tree model \n",
    "- Tune its hyperparameters \n",
    "- Evaluate its performance\n",
    "\n",
    "And describe what business value this model could provide in the real-world setting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fa89d5-7aad-4c6d-9e61-65d1d6647816",
   "metadata": {},
   "source": [
    "#### 1. Handle Missing Values\n",
    "- **Identify missing values** using `df.isnull().sum()` for a clear picture.\n",
    "- **Numerical features:**\n",
    "  - Use **mean/median imputation** for continuous data.\n",
    "  - For skewed distributions, prefer `median` to avoid distortion.\n",
    "- **Categorical features:**\n",
    "  - Replace missing entries with the **mode** or a distinct label like `\"Missing\"`.\n",
    "- Consider using **`SimpleImputer`** from `sklearn.impute` for streamlined preprocessing.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Encode Categorical Features\n",
    "- **Label Encoding**: For ordinal features with a natural order.\n",
    "- **One-Hot Encoding**: For nominal features (non-ordered), using `pd.get_dummies()` or `OneHotEncoder`.\n",
    "- If cardinality is high, consider **target encoding** or **hash encoding** to prevent feature explosion.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Train a Decision Tree Model\n",
    "```python\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "- No need to scale features, which is a bonus with tree-based models.\n",
    "- Decision trees naturally handle mixed data types after encoding.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Tune Hyperparameters\n",
    "Use **GridSearchCV** or **RandomizedSearchCV** to optimize parameters like:\n",
    "- `max_depth`: Controls overfitting.\n",
    "- `min_samples_split` and `min_samples_leaf`: Improve generalization.\n",
    "- `criterion`: Try both `'gini'` and `'entropy'`.\n",
    "- `class_weight`: Important if your dataset is imbalanced.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "grid_search = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_model = grid_search.best_estimator_\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. Evaluate Performance\n",
    "Use a mix of metrics:\n",
    "- **Accuracy**: Good for balanced datasets.\n",
    "- **Precision/Recall/F1 Score**: Crucial for disease prediction where false negatives matter.\n",
    "- **Confusion Matrix** and **ROC AUC**: For visual insights.\n",
    "```python\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "print(classification_report(y_test, best_model.predict(X_test)))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, best_model.predict_proba(X_test)[:,1]))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### Business Value in Healthcare\n",
    "A well-tuned model like this could:\n",
    "- **Enable early detection** and intervention for at-risk patients.\n",
    "- Support **personalized treatment plans**.\n",
    "- Help hospitals **prioritize resources** and manage patient loads.\n",
    "- Provide **predictive insights** for chronic condition management.\n",
    "- Reduce operational costs by **automating screening** and triage.\n",
    "\n",
    "By making disease prediction more proactive, the company could increase patient survival rates and reduce long-term care expenses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
